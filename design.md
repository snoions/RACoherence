# Release-Acquire Coherence

## Motivation

The CXL interconnect protocol, in particular the CXL 3.0 standard, enables multiple nodes to access a shared memory pool in a cache-coherent manner. However due to the cost of maintaining coherence over the large shared CXL memory pool, implementations of CXL 3.0 likely will only support cache-coherence over a small region of memory, while the rest of the larger shared memory pool would have no hardware-supported cache coherence. In short, the CXL shared memory will be divided into two regions:
- HC (hardware coherent) region of up to tens of TBs
- NHC (non-hardware coherent) region of a few GBs
To share data across nodes in the NCH region without consistency issues, any node that writes a cache line in the region would have to explicitly write back the cache line to memory (let's call it post-writeback) before other nodes read or partially write it. Note that partial writes are problematic with stale cache lines as the unwritten stale part may be written back, depending on the cache implementation. A more vexing problem is that the node reading or partially writing remotely modified cache lines also has to invalidate its local copy beforehand (let's call it pre-invalidate), and there is no reliable way for the software to know whether some data is cached, because of mechanisms like cache prefetching and branch predictions. For the NCH region, post-writeback is unavoidable, but repeated writeback of the same cache line may be avoided. Avoiding pre-invalidates can be more subtle as they depend on each node's knowledge about remotely modified cache lines.
In the most naive approach, a node has no idea which cache line has been remotely modified. It needs to write back after each write and invalidate before read/partial write to each cache line, which significantly reduces benefits of caching. This is the baseline that our protocol compares against. To avoid unnecessary pre-invalidates as much as possible, a node needs to
Be aware of when it may safely access cache lines without worrying about concurrent modifications by other nodes.
Notify other nodes about cache lines it has modified after they have been written back.
For (1) we leverage synchronization points and assumptions about race-freedom, and for (2) we design protocols for communicating cache line modifications between nodes using the CXL-CH regions.

## Leveraging Release-Acquire Synchronizations

For requirement (1), we observe that the commonly assumed data-race-freedom model can be helpful. If we put all atomic data used for synchronization into HC regions, then data-race freedom makes sure no concurrent modification to non-atomic data is possible. Since the basic unit of coherence is a cache line, we assume a stronger model of cache-line-race-freedom, i.e. no concurrent modifications to non-atomic cache lines. One conservative approach to ensure this is to remove all false sharing of cache lines on top of data-race-freedom. 

As data-race freedom is established through synchronization actions such as acquire loads and release stores, we treat them as synchronization points for cache coherence messages as well. Each node must publish the addresses modified cache lines in the form of logs before a release write, whereas on acquire read of some release write, the node need to know about all remote cache line modifications that happens before the release write according to happens-before order from the data-race free model, by having received updates about remotely modified cache lines. 

The happens-before order is tracked through timestamps attached to logs, such as vector clocks. Each node maintains a release timestamp, while each thread and each atomic location has their own clocks. The thread clocks are private to the thread and represents its view of the memory, while the (atomic) location clocks are stored as metadata in the HC region, and carry the view of the last thread that wrote to it . On a release write, a thread syncs its clock with node’s release timestamp and updates the clock of the stored location, and on an acquired load, the thread updates its clock with the clock of the loaded location. These steps essentially implement an operational semantics for acquire-release synchronization. In our protocol, each node additionally maintains a cache clock representing its progress on consuming logs of other nodes. The cache clock is incremented when it has received all updates before a remote release write, with the logs also following the happens before order. If it is less than the clock of an acquired location, the acquiring thread may not proceed until its node has consumed all the requisite updates.
Publishing Modified Cache Lines
Requirement (2) means the usual cache line writebacks and invalidates now coupled with the needs to respectively push/pull cache line addresses into/out of shared memory regions. Each of these can be controlled by separate mechanisms - a writeback policy decides how to record modified cache lines, write them back to memory, and publish them to other nodes, and an invalidate policy decides how to receive updates from other notes, and invalidate remotely modified cache lines before they are accessed. These policies have to work well together to ensure the performance of the system. 

On the push side, the release-acquire model naturally supports batch processing, which reduces the amount of cross-node communication and provides opportunity to avoid redundant writebacks in the same batch. In the most extreme case, a node can accumulate all logs of modified cache lines of a thread locally in a dirty cache line set, then write back and publish all modified cache lines at a release write. In practice, it may be better to publish logs incrementally, when the number of modified cache lines exceeds a certain threshold, in order to keep other nodes relatively up-to-date about the publisher’s progress, so they don’t have to stall when they read the release. This has some similarity to the hybrid flushing approach from the Atlas system for persistent memory programming. Within a batch, consecutive modified cache lines may be compressed into a more space-efficient range-based representation, though this also comes with trade-offs, as tracking long ranges of consecutive modified cache lines is at odds with incremental publishing, and incurs runtime cost that is not repaid when writes are not to consecutive cache lines.

The logs of modified cache lines are published through a broadcast queue inside CXL shared memory assigned to each node, which all other nodes read from. One subtle point to note here is that for multiple release writes to the same locations, the order of the logs in the channel, the order of writes, and the order of the atomic location clock update must be consistent with each other to ensure correctness. For efficient cross-node sharing, the broadcast queues are either entirely located inside HC regions, or store their metadata inside HC regions to trade off the need to writeback/invalidates log entries with the increased capacity of the NHC region. Memory capacity may be important here as HC regions have limited size, and logs may be garbage-collected only when they are read by all nodes other than the publisher.

On the pull side, each node runs a dedicated cache agent thread to ensure user threads can run unobstructed as much as possible. It scans the broadcast channel of other nodes for new logs according a certain scan policy, and processes the logs by either invalidates the logged cache lines eagerly, or record them in a node-private invalidate cache line directory that the user threads checks to lazily invalidate a cache line immediately before accessing it. Here the trade-off is between having user threads to potentially wait for a large number of eager invalidations to finish on an acquire load versus the cost of frequently checking a directory shared between all threads of the same node. In general, the lazy approach may be better for large amounts of modified cache lines released at the same time, which may come from the range-based cache line representation from the push side. It is also better for cache lines that are rarely or never accessed on the node, though identifying them requires profiling information collected by the runtime. If the invalidate cache line directory grows too large, all the cache lines can be invalidated at once with an instruction like x86’s wbinvd.

## Summary of Runtime System Behavior

In summary, the runtime system does the following on user program actions
- on plain store: pre-invalidate based on CL directory, store, update dirty-CL set
- on release store: publish log, increment release timestamp, update thread clock, update atomic location clock, release store
- on plain load: pre-invalidate based on CL directory, store
- on acquire load: wait until cache clock is no less than atomic location clock, acquire load, update thread lock.
while a per-node cache agent runs in the background.

## Advantage over Hardware Coherence

- (TODO) Enables compiler optimizations on logging and flush/invalidation sites
- (TODO) Enables adjusting cache policies, such as whether to flush eagerly or lazily, based on software information 

## Evaluation Results
TODO
